---
title: "p8105_hw5_kpp2126"
author: "Kevin P. Patterson"
date: "2022-11-13"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(ggplot2)
library(patchwork)
library(dplyr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

## Problem 1
1. Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time
*Start with a dataframe containing all file names; the `list.files` function will help
Iterate over file names and read in data for each subject using `purrr::map` and saving the result as a new variable in the dataframe
*Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary

```{r load, tidy data, warning=FALSE, message=FALSE}
longitudinal_df = 
  tibble(
    files = list.files("./data/p1_data/"), #listing the "./" method due to inability to read with "/data" alone
    path = str_c("./data/p1_data/", files)
  ) %>% 
  mutate(data = purrr::map(path, read_csv)) %>% 
  unnest()

#tidy df
longtidy_df = 
  longitudinal_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

2. Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.
```{r}
Spaghetti_plot = longtidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
Spaghetti_plot
```

## Problem 2

```{r load raw data, message=FALSE}
rawhom_df =
  read_csv("data/homicide_data.csv")
```
1. Describe the raw data. 
The resulting `rawhom_df` raw dataframe has `r nrow(rawhom_df)` observations and `r ncol(rawhom_df)` variables. These `r ncol(rawhom_df)` variables detail individual homicide demographic information such as `uid`, `reported_date`, victim's first and last name, race, age, sex, city, state, location indicators latitude & longitude, and disposition. 

2. Create a `city_state` variable (e.g. “Baltimore, MD”) using `mutate` and then;
3. `Summarize` within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
```{r tidy data}
tidyhom_df = rawhom_df %>%
  mutate(city_state = str_c(city, state, sep = ", "), #create city_state variable
         disposition = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), "unsolved", disposition)) %>% #create unsolved vs Closed by arrest distinction within the disposition variable
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition == "unsolved") #create the summarized variables
  ) %>%
  arrange(desc(total_homicides)) #arranging by descending order to better visualize the count data

#fixing Milwaukee, wI to WI
tidyhom_df = tidyhom_df %>%
  mutate(city_state = ifelse(city_state %in% c("Milwaukee, wI"), "Milwaukee, WI", city_state)) 

knitr::kable(head(tidyhom_df[, 1:3]), "simple")
```


4. For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; 
- save the output of prop.test as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```{r prop test Baltimore}
estimate = 
  prop.test(
  tidyhom_df %>%
    filter(city_state == "Baltimore, MD") %>%
    pull(unsolved_homicides),
  tidyhom_df %>%
    filter(city_state == "Baltimore, MD") %>%
    pull(total_homicides))

broom::tidy(estimate)
```

The estimated proportion (95% CI) of homicides that are unsolved for the city of Baltimore, MD is `r round(estimate$estimate, digits=3)` (`r round(estimate$conf.int[1], digits=3)`, `r round(estimate$conf.int[2], digits=3)`).

5. Now run `prop.test` for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. 
- Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r prop test cities}
tidycity_df =
  tidyhom_df %>%
  mutate(
    proptest_outputs = purrr::map2(.x = unsolved_homicides, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    estimate_outputs = purrr::map(.x = proptest_outputs, ~broom::tidy(.x))
  ) %>% 
  select(-proptest_outputs) %>% 
  unnest(estimate_outputs) %>% 
  select(city_state, estimate, conf.low, conf.high, p.value) %>% #included p.value as an additional context for the estimate
  arrange(desc(estimate))
```

6. Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.
```{r plot}
tidycity_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high))
```

## Problem 3
First set the following design elements:
Fix n=30
Fix σ=5
Set μ=0. Generate 5000 datasets from the model

x∼Normal[μ,σ]

For each dataset, save μ̂  and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

```{r}

```


